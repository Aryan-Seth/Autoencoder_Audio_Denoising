{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOgC2545nUNiYVFuFSdsfSa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQhKB-E7BVn0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from matplotlib import pyplot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchbnn as bnn\n",
        "\n",
        "\n",
        "def do(x_train, y_train, x_test, y_test, model):\n",
        "    model.fit(x_train, y_train.ravel())\n",
        "    preds = model.predict(x_test)\n",
        "    err = mean_squared_error(y_test, preds)**0.5\n",
        "    sco = r2_score(preds,y_test)\n",
        "    return err, sco\n",
        "# returns mse on selected model\n",
        "\n",
        "\n",
        "def call(a):\n",
        "    if a == 1:\n",
        "        model = LinearRegression()\n",
        "        label = 'Linear Reg'\n",
        "    if a == 2:\n",
        "        model = SVR(C=170, gamma=5, epsilon=8)\n",
        "        label = 'SVR'\n",
        "    if a == 3:\n",
        "        model = BayesianRidge()\n",
        "        label = 'BayesianRidge'\n",
        "    if a == 4:\n",
        "        model = RandomForestRegressor()\n",
        "        label = 'RFR'\n",
        "    if a == 5:\n",
        "        model = DecisionTreeRegressor(max_depth=10)\n",
        "        label = 'DTR'\n",
        "    if a == 6:\n",
        "        model = GradientBoostingRegressor(n_iter_no_change=10)\n",
        "        label = 'GBR'\n",
        "\n",
        "    return label, model\n",
        "# chooses model based on input and outputs label for plotting\n",
        "\n",
        "\n",
        "def plotall(err, labels, n):\n",
        "    x_pos = np.arange(len(labels))\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bar(x_pos, err, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
        "    ax.set_ylabel('MSE per model')\n",
        "    ax.set_xticks(x_pos)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.set_title('error for iterations= '+str(n))\n",
        "    ax.yaxis.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# plots mse of all selected models on given dataset\n",
        "\n",
        "\n",
        "def dataclean(df):\n",
        "    val = (df.iat[i, 0] for i in range(df.shape[0]))\n",
        "    val = list(val)\n",
        "\n",
        "    ord = []\n",
        "\n",
        "    col = [i for i in range(0, len(df.columns))]\n",
        "    df.columns = col\n",
        "    col = list(col)\n",
        "\n",
        "    intdata = df.select_dtypes(\"int\")\n",
        "    cont = df.select_dtypes(\"float\")\n",
        "\n",
        "    a = list(df.columns)\n",
        "\n",
        "    for i in cont:\n",
        "        b = pd.unique(df[i])\n",
        "        for j in b:\n",
        "            if(j >= 0):\n",
        "                flag = 1\n",
        "            else:\n",
        "                flag = 0\n",
        "                break\n",
        "        if(flag == 0):\n",
        "            print(str(i) + \" is interval\")\n",
        "        else:\n",
        "            print(str(i)+\" is ratio\")\n",
        "\n",
        "    print(a)\n",
        "    for l in a:\n",
        "        p = len(pd.unique(df[l]))\n",
        "        if p < 10:\n",
        "            ord.append(l)\n",
        "\n",
        "    end = int(len(a)-1)\n",
        "    Y = df.iloc[:, end]\n",
        "    df = df.drop([end], axis=1)\n",
        "    print(Y)\n",
        "\n",
        "    extracol = df.nunique(axis=1)\n",
        "    sum_uniq = 0\n",
        "    for i in range(len(extracol)):\n",
        "        sum_uniq = sum_uniq+extracol[i]\n",
        "    for i in ord:\n",
        "        pd.concat([df, pd.get_dummies(df.iloc[:, i])], axis=1)\n",
        "        df.drop([i])\n",
        "\n",
        "    col = [i for i in range(0, len(df.columns))]\n",
        "    df.columns = col\n",
        "    print(df.head())\n",
        "    imputer = KNNImputer(n_neighbors=3)\n",
        "\n",
        "    X = imputer.fit_transform(df)\n",
        "    df = pd.DataFrame(X)\n",
        "    # df=normalize(df)\n",
        "    df = pd.DataFrame(df)\n",
        "    return Y, df\n",
        "# converts ordinal data to one hot encoded, checks between ratio and interval data and imputes missing values\n",
        "\n",
        "\n",
        "def fsps(x_train, y_train, x_test, model):\n",
        "\n",
        "    sfs = SequentialFeatureSelector(model, n_features_to_select='auto')\n",
        "    sfs.fit(x_train, y_train)\n",
        "    print(sfs.get_support())\n",
        "    x_train = sfs.transform(x_train)\n",
        "    x_test = sfs.transform(x_test)\n",
        "    return x_train, x_test\n",
        "# implements feature selection for dataset after datacleaning\n",
        "\n",
        "\n",
        "def histgrad(model, x_train, y_train, x_test, y_test):\n",
        "    model.fit(x_train, y_train, sample_weight=None)\n",
        "    preds = model.predict(x_test)\n",
        "    err = mean_squared_error(y_test, preds)**0.5\n",
        "    sco = model.score(y_test, preds)\n",
        "    return err, sco\n",
        "\n",
        "\n",
        "def main():\n",
        "    df = pd.read_csv(\"C:\\\\Users\\\\Aryan Seth\\\\Downloads\\\\supplementary_material_ilthermodataset_all.csv\")\n",
        "    # Y,df=dataclean(df)\n",
        "    Y = df['Young']\n",
        "    df.drop(['Young'], axis=1)\n",
        "    imputer = KNNImputer(n_neighbors=3)\n",
        "    # X = imputer.fit_transform(df)\n",
        "    # df = pd.DataFrame(X)\n",
        "    # print(df.head())\n",
        "    # print(df.columns.values)\n",
        "    df = normalize(df)\n",
        "    df = pd.DataFrame(df)\n",
        "\n",
        "    # print([df.iloc[:, i] for i in range(7, 10)])\n",
        "    # print(Y.head())\n",
        "    x_train, x_test, y_train, y_test = train_test_split(df, Y, test_size=0.3)\n",
        "    y_train = y_train.values.reshape(-1, 1)\n",
        "    # model = LinearRegression()\n",
        "    # print(df.head())\n",
        "    df = df.drop([0], axis=1)\n",
        "    # x_train, x_test = fsps(x_train, y_train, x_test, model)\n",
        "\n",
        "    k = int(input(\"how many models do you want to run?\\n\"))\n",
        "    print(\"input 1 for ...\")\n",
        "\n",
        "    e = []\n",
        "    s = []\n",
        "    for j in range(0, 1):\n",
        "        for i in range(0, k):\n",
        "            model = None\n",
        "            a = int(input())\n",
        "            if a == 9:\n",
        "                model = HistGradientBoostingRegressor\n",
        "                histgrad(model, x_train, y_train, x_test, y_test)\n",
        "            label1, model = call(a, x_train, y_train, x_test, y_test)\n",
        "            er, sco = do(x_train, y_train, x_test, y_test, model)\n",
        "            e.append(er)\n",
        "            s.append(sco)\n",
        "        \n",
        "    print(\"test error= \")\n",
        "    print(e)\n",
        "    print(\"r2= \")\n",
        "    print(s)\n",
        "    # plotall(err, label, k)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def deep():\n",
        "    df = pd.read_csv()\n",
        "    Y, df = dataclean(df)\n",
        "    print(df.head())\n",
        "    x_train, x_test, y_train, y_test = train_test_split(df, Y, test_size=0.2)\n",
        "    model = nn.Sequential(\n",
        "        bnn.BayesLinear(prior_mu=0, prior_sigma=0.1,\n",
        "                        in_features=19, out_features=500),\n",
        "        nn.ReLU(),\n",
        "        bnn.BayesLinear(prior_mu=0, prior_sigma=0.1,\n",
        "                        in_features=500, out_features=200),\n",
        "        nn.ReLU(),\n",
        "        bnn.BayesLinear(prior_mu=0, prior_sigma=0.1,\n",
        "                        in_features=200, out_features=100),\n",
        "        nn.ReLU(),\n",
        "        bnn.BayesLinear(prior_mu=0, prior_sigma=0.1,\n",
        "                        in_features=100, out_features=20),                \n",
        "        nn.ReLU(),\n",
        "        bnn.BayesLinear(prior_mu=0, prior_sigma=0.1,\n",
        "                        in_features=20, out_features=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    model.double()\n",
        "    new_shape = (613, 1)\n",
        "\n",
        "    mse_loss = nn.MSELoss()\n",
        "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
        "    kl_weight = 0.01\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    kl_weight = 0.1\n",
        "\n",
        "    x_train = torch.tensor(x_train.values)\n",
        "    y_train = torch.tensor(y_train.values)\n",
        "    # x_train = x_train.view(new_shape)\n",
        "    y_train = y_train.view(new_shape)\n",
        "    for step in range(0, 20000):\n",
        "        pre = model(x_train)\n",
        "        mse = mse_loss(pre, y_train)\n",
        "        kl = kl_loss(model)\n",
        "        cost = mse + kl_weight*kl\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('- MSE : %2.2f, KL : %2.2f' % (mse.item(), kl.item()))\n",
        "\n",
        "# deep()\n",
        "\n",
        "def plot():\n",
        "    import seaborn as sns\n",
        "    df = pd.read_csv(\"\")\n",
        "    Y, df = dataclean(df)\n",
        "    df = df.iloc[:, 0:7]\n",
        "    df = pd.concat([df, Y], axis=1)\n",
        "    print(df.head())\n",
        "    df.columns = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
        "    print(df.head())\n",
        "    g = sns.PairGrid(df, palette=True)\n",
        "    g.map(sns.scatterplot)\n",
        "    g.add_legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def svroptim():\n",
        "    df = pd.read_csv(\"\")\n",
        "    # Y,df=dataclean(df)\n",
        "    Y = df['Young']\n",
        "    df.drop(['Young'], axis=1)\n",
        "    imputer = KNNImputer(n_neighbors=3)\n",
        "    X = imputer.fit_transform(df)\n",
        "    df = pd.DataFrame(X)\n",
        "    df = normalize(df)\n",
        "    df = pd.DataFrame(df)\n",
        "\n",
        "    svr = SVR()\n",
        "    param_grid = {\n",
        "        'C': [1.1, 5.4, 170, 1001],\n",
        "        'epsilon': [0.0003, 0.007, 0.0109, 0.019, 0.14, 0.05, 8, 0.2, 3, 2, 7],\n",
        "        'gamma': [0.7001, 0.008, 0.001, 3.1, 1, 1.3, 5]\n",
        "    }\n",
        "    grid_search = GridSearchCV(estimator=svr, cv=5,    param_grid={\n",
        "        'C': [1.1, 5.4, 170, 1001],\n",
        "        'epsilon': [0.0003, 0.007, 0.0109, 0.019, 0.14, 0.05, 8, 0.2, 3, 2, 7],\n",
        "        'gamma': [0.7001, 0.008, 0.001, 3.1, 1, 1.3, 5]\n",
        "    }, n_jobs=-1, verbose=2)\n",
        "    grid_search.fit(df, Y)\n",
        "    print(grid_search.best_params_)\n",
        "# svroptim()\n",
        "\n",
        "\n",
        "def featimp():\n",
        "    df = pd.read_csv(\"\")\n",
        "    # Y,df=dataclean(df)\n",
        "    Y = df['Young']\n",
        "    df=df.drop(['Young'], axis=1)\n",
        "    imputer = KNNImputer(n_neighbors=3)\n",
        "    X = imputer.fit_transform(df)\n",
        "    df = pd.DataFrame(X)\n",
        "    df = normalize(df)\n",
        "    df = pd.DataFrame(df)\n",
        "    print(df.columns.values)\n",
        "    model = LinearRegression()\n",
        "# fit the model\n",
        "    model.fit(df, Y)\n",
        "# get importance\n",
        "    importance = model.coef_\n",
        "# summarize feature importance\n",
        "    for i, v in enumerate(importance):\n",
        "        print('Feature: %0d, Score: %.5f' % (i, v))\n",
        "    \n",
        "    \n",
        "    pyplot.bar([x for x in range(len(importance))], importance)\n",
        "    pyplot.xticks([x for x in range(len(importance))],df.columns.values)\n",
        "    pyplot.show()\n"
      ]
    }
  ]
}